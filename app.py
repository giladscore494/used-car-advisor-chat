# -*- coding: utf-8 -*-
# UsedCarAdvisor â€“ Structured Questionnaire + Free-text input
# Run: streamlit run app.py

import os, json, re
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
import streamlit as st

try:
    from openai import OpenAI
except Exception:
    OpenAI = None

try:
    import google.generativeai as genai
except Exception:
    genai = None

st.set_page_config(page_title="×™×•×¢×¥ ×¨×›×‘×™× ×™×“ 2 â€“ ×©××œ×•×Ÿ + ×—×•×¤×©×™", page_icon="ğŸ¤–ğŸš—", layout="centered")

RTL = """
<style>
html, body, [class*="css"] { direction: rtl; text-align: right; }
.block-container { padding-top: .6rem; max-width: 880px; }
.stChatMessage { text-align: right; }
</style>
"""
st.markdown(RTL, unsafe_allow_html=True)

# ×›×¤×ª×•×¨ ××™×¤×•×¡
if st.sidebar.button("ğŸ”„ ×”×ª×—×œ ××—×“×©"):
    for key in list(st.session_state.keys()):
        del st.session_state[key]
    st.rerun()

# ×¡×¤×§ ××•×“×œ
PROVIDER = st.sidebar.selectbox("×¡×¤×§ ××•×“×œ", ["OpenAI", "Gemini"], index=0)
openai_key = os.getenv("OPENAI_API_KEY", "")
gemini_key = os.getenv("GEMINI_API_KEY", "") or os.getenv("GOOGLE_API_KEY", "")

if PROVIDER == "OpenAI":
    has_key = bool(openai_key and OpenAI)
    model_name = st.sidebar.text_input("OpenAI Model", value="gpt-4.1-mini")
    oai_client = OpenAI(api_key=openai_key) if has_key else None
else:
    has_key = bool(gemini_key and genai)
    model_name = st.sidebar.text_input("Gemini Model", value="gemini-1.5-flash")
    if has_key:
        genai.configure(api_key=gemini_key)
        gem_model = genai.GenerativeModel(model_name)
    else:
        gem_model = None

st.sidebar.markdown(f"**×¡×˜×˜×•×¡ ×¡×¤×§:** {'âœ… ××—×•×‘×¨' if has_key else 'âŒ ×œ×œ× ××¤×ª×—/×¡×¤×¨×™×”'}")

# ××¦×‘×™×
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role":"assistant","content":"×”×™×™! × ×ª×—×™×œ ×‘×©××œ×•×Ÿ, ××‘×œ ××¤×©×¨ ×’× ×œ×›×ª×•×‘ ×—×•×¤×©×™ (×œ××©×œ: '×‘× ×œ×™ ×—×™×™×ª ×›×‘×™×© ××™×˜×œ×§×™×ª ×¢× ×˜×•×¨×‘×• ×¢×“ 80 ××œ×£')."}
    ]
if "answers" not in st.session_state:
    st.session_state.answers = {}
if "last_ask" not in st.session_state:
    st.session_state.last_ask = None

# ×©××œ×•×Ÿ
@dataclass
class Slot:
    key: str
    label: str
    prompt: str
    kind: str
    required: bool = True

SLOTS: List[Slot] = [
    Slot("budget_min", "×ª×§×¦×™×‘ ××™× ×™××•× (â‚ª)", "××” ×”×ª×§×¦×™×‘ ×”××™× ×™××œ×™ ×©×œ×š ×‘×©×§×œ×™×?", "int"),
    Slot("budget_max", "×ª×§×¦×™×‘ ××§×¡×™××•× (â‚ª)", "×•××” ×”××§×¡×™××•× ×©××ª×” ××•×›×Ÿ ×œ×©×œ×?", "int"),
    Slot("body", "×¡×•×’ ×¨×›×‘", "××™×–×” ×¡×•×’ ×¨×›×‘ ××ª×” ××—×¤×©? (××©×¤×—×ª×™, ×”××¦'×‘×§, ×’'×™×¤...)", "text"),
    Slot("character", "××•×¤×™ ×¨×›×‘", "×”×¢×“×¤×”: ×¡×¤×•×¨×˜×™×‘×™ ××• ×™×•××™×•××™?", "text"),
    Slot("fuel", "×¡×•×’ ×“×œ×§", "××™×–×” ×¡×•×’ ×“×œ×§ ×ª×¢×“×™×£ â€“ ×‘× ×–×™×Ÿ, ×“×™×–×œ, ×”×™×‘×¨×™×“×™, ×—×©××œ×™?", "text"),
    Slot("year_min", "×©× ×ª ×™×™×¦×•×¨ ××™× ×™××œ×™×ª", "×××™×–×• ×©× ×ª ×™×™×¦×•×¨ ××™× ×™××œ×™×ª ×ª×¨×¦×”?", "int"),
    Slot("engine_size", "× ×¤×— ×× ×•×¢", "××™×–×” × ×¤×— ×× ×•×¢ ×‘×¢×¨×š ××ª××™× ×œ×š? (×œ××©×œ 1600)", "int"),
    Slot("turbo", "×˜×•×¨×‘×•", "×”×× ×—×©×•×‘ ×œ×š ×˜×•×¨×‘×•?", "text"),
]
REQUIRED_KEYS = [s.key for s in SLOTS if s.required]

# ×¤×•× ×§×¦×™×•×ª ×¢×–×¨
def call_model(prompt: str) -> str:
    try:
        if PROVIDER == "OpenAI" and has_key and oai_client:
            resp = oai_client.chat.completions.create(
                model=model_name,
                messages=[{"role":"user","content":prompt}],
                temperature=0.2,
            )
            return resp.choices[0].message.content
        elif PROVIDER == "Gemini" and has_key and gem_model:
            r = gem_model.generate_content(prompt)
            return r.text or ""
    except Exception as e:
        return f"(×©×’×™××” ×‘×§×¨×™××” ×œ××•×“×œ: {e})"
    return ""

def interpret_free_text(user_text: str) -> Dict[str, Any]:
    prompt = f"""
    ×”××©×ª××© ×›×ª×‘: "{user_text}"
    × ×ª×— ×–××ª ×œ×“×¨×™×©×•×ª ×¨×›×‘:
    - budget_min, budget_max (××¡×¤×¨×™× ×‘×©×§×œ×™× ×× ×™×©)
    - body (××©×¤×—×ª×™, ×”××¦'×‘×§, ×’'×™×¤, ×¡×“××Ÿ...)
    - character (×¡×¤×•×¨×˜×™×‘×™, ×™×•××™×•××™)
    - fuel (×‘× ×–×™×Ÿ, ×“×™×–×œ, ×”×™×‘×¨×™×“×™, ×—×©××œ×™)
    - turbo (×¢× ×˜×•×¨×‘×• / ×‘×œ×™ ×˜×•×¨×‘×•)
    - brand (××•×ª×’ ×× ×¦×•×™×Ÿ, ××—×¨×ª null)
    - engine_size (× ×¤×— ×× ×•×¢ ×× ×¦×•×™×Ÿ)

    ×”×—×–×¨ JSON ×‘×œ×‘×“.
    """
    txt = call_model(prompt)
    try:
        return json.loads(re.search(r"\{.*\}", txt, re.S).group())
    except Exception:
        return {}

def next_missing_required():
    for s in SLOTS:
        if s.key not in st.session_state.answers:
            return s
    return None

def progress_bar(answers: Dict[str,Any]):
    filled = sum(1 for k in REQUIRED_KEYS if k in answers and answers[k] not in [None,"",0])
    pct = int(100 * filled / len(REQUIRED_KEYS))
    st.markdown(f"**×”×ª×§×“××•×ª ×”×©××œ×•×Ÿ:** {pct}%")
    st.progress(pct)

# ×”×¦×’×ª ×¦'××˜
st.markdown("## ğŸ¤– ×™×•×¢×¥ ×¨×›×‘×™× â€“ ×©××œ×•×Ÿ + ×˜×§×¡×˜ ×—×•×¤×©×™")
progress_bar(st.session_state.answers)
for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

# ×§×œ×˜ ××©×ª××©
user_text = st.chat_input("×›×ª×•×‘ ×ª×©×•×‘×” ×—×•×¤×©×™×ª ××• ×œ×¤×™ ×”×©××œ×”...")
if user_text:
    st.session_state.messages.append({"role":"user","content":user_text})

    # ×× ×™×© ×©××œ×” ×¤×ª×•×—×” ×‘×©××œ×•×Ÿ â†’ × ×¢×“×›×Ÿ ×™×©×™×¨×•×ª
    if st.session_state.last_ask:
        st.session_state.answers[st.session_state.last_ask.key] = user_text.strip()
        st.session_state.last_ask = None

    # × ×™×ª×•×— ×˜×§×¡×˜ ×—×•×¤×©×™ â†’ ×”×›× ×¡×ª ×¢×¨×›×™×
    parsed = interpret_free_text(user_text)
    for k,v in parsed.items():
        if v not in [None,"",0,"null"]:
            st.session_state.answers[k] = v

    # ×× ×¢×“×™×™×Ÿ ×—×¡×¨×™× ×©×“×•×ª â†’ ×”××©×š ×©××œ×•×Ÿ
    nxt = next_missing_required()
    if nxt:
        st.session_state.last_ask = nxt
        with st.chat_message("assistant"):
            st.markdown(nxt.prompt)
        st.session_state.messages.append({"role":"assistant","content":nxt.prompt})
    else:
        # ×”×›×œ ××•×œ× â†’ ×¡×™×›×•× ×“×¨×™×©×•×ª + ×¤×¨×•××¤×˜ ×—×™×¤×•×©
        answers = st.session_state.answers
        summary = "### ×¡×™×›×•× ×“×¨×™×©×•×ª×™×š\n" + "\n".join([f"- {k}: {v}" for k,v in answers.items()])
        with st.chat_message("assistant"):
            st.markdown(summary)
        st.session_state.messages.append({"role":"assistant","content":summary})

        # ×—×™×¤×•×© ×¨×›×‘×™×
        search_prompt = f"""
        ×‘×”×ª×‘×¡×¡ ×¢×œ ×”×“×¨×™×©×•×ª: {json.dumps(answers, ensure_ascii=False)},
        ×‘×—×¨ 5 ×“×’××™ ×¨×›×‘×™× ×™×“ ×©× ×™×™×” ×”× ××›×¨×™× ×‘×™×©×¨××œ ×‘×œ×‘×“.
        ×× ×”××©×ª××© ×‘×™×§×© ×˜×•×¨×‘×• â€“ ××œ ×ª×¦×™×¢ ×“×’× ×‘×œ×™ ×˜×•×¨×‘×•.
        ×× ×¦×™×™×Ÿ ××•×ª×’ â€“ ×›×œ×•×œ ×¨×§ ×“×’××™× ×××•×ª×• ××•×ª×’.
        ×”×—×–×¨ JSON:
        {{"recommendations":[{{"model":"×“×’×","why":"× ×™××•×§ ×§×¦×¨"}}]}}
        """
        txt = call_model(search_prompt)
        try:
            recs = json.loads(re.search(r"\{.*\}", txt, re.S).group())
        except Exception:
            recs = {"recommendations":[]}

        table_md = "| ×“×’× | × ×™××•×§ |\n|---|---|\n"
        for r in recs.get("recommendations",[]):
            table_md += f"| {r['model']} | {r['why']} |\n"

        with st.chat_message("assistant"):
            st.markdown("### ×”×¦×¢×•×ª ×¨×›×‘×™× ××ª××™××•×ª\n" + table_md)
        st.session_state.messages.append({"role":"assistant","content":table_md})

st.markdown("---")
st.caption("×”××¤×œ×™×§×¦×™×” ××©×œ×‘×ª ×©××œ×•×Ÿ ××•×‘× ×” + ×”×‘× ×ª ×˜×§×¡×˜ ×—×•×¤×©×™. ×”×ª×©×•×‘×•×ª ×”×—×•×¤×©×™×•×ª ××•×–×¨×§×•×ª ×œ×¤×¨×•××¤×˜ ×›×“×™ ×œ×—×“×“ ××ª ×”×”××œ×¦×•×ª.")
