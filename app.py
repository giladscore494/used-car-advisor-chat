# -*- coding: utf-8 -*-
# UsedCarAdvisor â€“ ChatBot-First with In-Chat Questionnaire (Streamlit, single-file)
# Run: streamlit run app.py

import os
import json
import re
from dataclasses import dataclass
from typing import List, Dict, Any, Optional

import streamlit as st

try:
    from openai import OpenAI
except Exception:
    OpenAI = None

try:
    import google.generativeai as genai
except Exception:
    genai = None

st.set_page_config(page_title="×™×•×¢×¥ ×¨×›×‘×™× ×™×“ 2 â€“ ×¦'××˜ ×¢× ×©××œ×•×Ÿ ××•×˜××¢", page_icon="ğŸ¤–ğŸš—", layout="centered")

RTL = """
<style>
html, body, [class*="css"] { direction: rtl; text-align: right; }
.block-container { padding-top: .6rem; max-width: 880px; }
.stChatMessage { text-align: right; }
</style>
"""
st.markdown(RTL, unsafe_allow_html=True)

# =========================
# Questionnaire slots
# =========================
@dataclass
class Slot:
    key: str
    label: str
    prompt: str
    kind: str
    required: bool = True

SLOTS: List[Slot] = [
    Slot("budget_min", "×ª×§×¦×™×‘ ××™× ×™××•× (â‚ª)", "××” ×”×ª×§×¦×™×‘ ×”××™× ×™××œ×™ ×©×œ×š ×‘×©×§×œ×™×? (×œ×“×•×’××”: 40,000)", "int"),
    Slot("budget_max", "×ª×§×¦×™×‘ ××§×¡×™××•× (â‚ª)", "××” ×”×ª×§×¦×™×‘ ×”××§×¡×™××œ×™ ×©×œ×š ×‘×©×§×œ×™×? (×œ×“×•×’××”: 80,000)", "int"),
    Slot("body", "×¡×•×’ ×¨×›×‘", "××™×–×” ×¡×•×’ ×¨×›×‘ ××ª×” ××—×¤×©? (×œ×“×•×’××”: ××©×¤×—×ª×™, ×§×˜×Ÿ, ×’'×™×¤)", "text"),
    Slot("character", "××•×¤×™ ×¨×›×‘", "×”×× ××ª×” ××—×¤×© ×¨×›×‘ ×¡×¤×•×¨×˜×™×‘×™ ××• ×™×•××™×•××™?", "text"),
    Slot("usage", "×©×™××•×© ×¢×™×§×¨×™", "×”×©×™××•×© ×”×¢×™×§×¨×™ ×™×”×™×” ×‘×¢×™×¨, ×‘×™×Ÿ-×¢×™×¨×•× ×™ ××• ×©×˜×—?", "text"),
    Slot("priority", "×¢×“×™×¤×•×ª ××¨×›×–×™×ª", "××” ×”×›×™ ×—×©×•×‘ ×œ×š â€“ ×××™× ×•×ª, × ×•×—×•×ª, ×‘×™×¦×•×¢×™× ××• ×¢×™×¦×•×‘?", "text"),
    Slot("passengers", "××¡×¤×¨ × ×•×¡×¢×™× ×××•×¦×¢", "×‘×××•×¦×¢ ×›××” × ×•×¡×¢×™× ×™×™×¡×¢×• ×‘×¨×›×‘? (×œ×“×•×’××”: 5)", "int"),
    Slot("fuel", "×¡×•×’ ×“×œ×§", "××™×–×” ×¡×•×’ ×“×œ×§ ×ª×¢×“×™×£ â€“ ×‘× ×–×™×Ÿ, ×“×™×–×œ, ×”×™×‘×¨×™×“×™ ××• ×—×©××œ×™?", "text"),
    Slot("year_min", "×©× ×ª ×™×™×¦×•×¨ ××™× ×™××œ×™×ª", "×××™×–×• ×©× ×ª ×™×™×¦×•×¨ ××™× ×™××œ×™×ª ×ª×¨×¦×”? (×œ×“×•×’××”: 2015)", "int"),
    Slot("km_per_year", "×§\"× ×œ×©× ×”", "×›××” ×§×™×œ×•××˜×¨×™× ××ª×” × ×•×¡×¢ ×‘×¢×¨×š ×‘×©× ×”? (×œ×“×•×’××”: 15000)", "int"),
    Slot("gearbox", "×ª×™×‘×ª ×”×™×œ×•×›×™×", "×™×© ×œ×š ×”×¢×“×¤×” ×œ×’×™×¨ â€“ ××•×˜×•××˜ ××• ×™×“× ×™?", "text"),
    Slot("region", "××–×•×¨ ×‘××¨×¥", "×‘××™×–×” ××–×•×¨ ×‘××¨×¥ ××ª×” ×’×¨?", "text"),
    # ×—×“×©×™×:
    Slot("engine_size", "× ×¤×— ×× ×•×¢", "××” × ×¤×— ×”×× ×•×¢ ×”××•×¢×“×£ ×¢×œ×™×š? (×œ×“×•×’××”: 1600)", "int"),
    Slot("turbo", "×˜×•×¨×‘×•", "×”×× ××ª×” ××—×¤×© ×× ×•×¢ ×¢× ×˜×•×¨×‘×• ××• ×‘×œ×™ ×˜×•×¨×‘×•?", "text"),
]
REQUIRED_KEYS = [s.key for s in SLOTS if s.required]

# =========================
# App state
# =========================
if "messages" not in st.session_state:
    st.session_state.messages: List[Dict[str, str]] = [
        {"role":"assistant","content":"×”×™×™! ×× ×™ ×”×™×•×¢×¥ ×œ×¨×›×‘×™× ×™×“ 2. × ×ª×—×™×œ ×‘×©××œ×” ×§×¦×¨×” â€“ ××” ×”×ª×§×¦×™×‘ ×”××™× ×™××œ×™ ×©×œ×š ×‘×©×§×œ×™×? (×œ×“×•×’××”: 40,000)"}
    ]
if "answers" not in st.session_state:
    st.session_state.answers: Dict[str, Any] = {}
if "last_ask" not in st.session_state:
    st.session_state.last_ask = None

# =========================
# Provider setup
# =========================
PROVIDER = st.sidebar.selectbox("×¡×¤×§ ××•×“×œ", ["OpenAI", "Gemini"], index=0)
openai_key = os.getenv("OPENAI_API_KEY", "")
gemini_key = os.getenv("GEMINI_API_KEY", "") or os.getenv("GOOGLE_API_KEY", "")

if PROVIDER == "OpenAI":
    has_key = bool(openai_key and OpenAI)
    model_name = st.sidebar.text_input("OpenAI Model", value="gpt-4.1-mini")
    oai_client = OpenAI(api_key=openai_key) if has_key else None
else:
    has_key = bool(gemini_key and genai)
    model_name = st.sidebar.text_input("Gemini Model", value="gemini-1.5-flash")
    if has_key:
        genai.configure(api_key=gemini_key)
        gem_model = genai.GenerativeModel(model_name)
    else:
        gem_model = None

st.sidebar.markdown(f"**×¡×˜×˜×•×¡ ×¡×¤×§:** {'âœ… ××—×•×‘×¨' if has_key else 'âŒ ×œ×œ× ××¤×ª×—/×¡×¤×¨×™×”'}")

# =========================
# Helpers
# =========================
def parse_int(text: str) -> Optional[int]:
    nums = re.findall(r"\d+", text.replace(",", ""))
    if nums:
        try:
            return int(nums[0])
        except Exception:
            return None
    return None

def next_missing_required() -> Optional[Slot]:
    for s in SLOTS:
        if s.required and (s.key not in st.session_state.answers or st.session_state.answers[s.key] in [None,"",0,""]):
            return s
    return None

def call_model(prompt: str) -> str:
    try:
        if PROVIDER == "OpenAI" and has_key and oai_client:
            resp = oai_client.chat.completions.create(
                model=model_name,
                messages=[{"role":"user","content":prompt}],
                temperature=0.3,
            )
            return resp.choices[0].message.content
        elif PROVIDER == "Gemini" and has_key and gem_model:
            r = gem_model.generate_content(prompt)
            return r.text or ""
    except Exception as e:
        return f"(×©×’×™××” ×‘×§×¨×™××” ×œ××•×“×œ: {e})"
    return "(××™×Ÿ ×—×™×‘×•×¨ ×œ××•×“×œ)"

# =========================
# Display history
# =========================
st.markdown("## ğŸ¤– ×™×•×¢×¥ ×¨×›×‘×™× â€“ ×¦'××˜ ×¢× ×©××œ×•×Ÿ")
for m in st.session_state.messages:
    with st.chat_message("assistant" if m["role"]=="assistant" else "user"):
        st.markdown(m["content"])

# =========================
# Chat input
# =========================
user_text = st.chat_input("×›×ª×•×‘ ×ª×©×•×‘×” ×›××Ÿ ×•×”×§×© ×× ×˜×¨...")

if user_text:
    st.session_state.messages.append({"role":"user","content":user_text})
    # ×©××™×¨×” ×œ×ª×•×š ×ª×©×•×‘×•×ª
    if st.session_state.get("last_ask"):
        slot = st.session_state.last_ask
        if slot.kind == "int":
            val = parse_int(user_text)
            if val: st.session_state.answers[slot.key] = val
        else:
            st.session_state.answers[slot.key] = user_text.strip()
        st.session_state.last_ask = None

    # ××¦×™××ª ×”×©××œ×” ×”×‘××”
    nxt = next_missing_required()
    if nxt:
        st.session_state.last_ask = nxt
        with st.chat_message("assistant"):
            st.markdown(nxt.prompt)
        st.session_state.messages.append({"role":"assistant","content":nxt.prompt})
    else:
        # === ×›×œ ×”×©××œ×•×Ÿ ××•×œ× ===
        answers = st.session_state.answers
        with st.chat_message("assistant"):
            st.markdown("âœ… ×¡×™×™×× ×• ××ª ×©×œ×‘ ×”×©××œ×•×Ÿ. ××—×¤×© ×¨×›×‘×™× ××ª××™××™×...")

        # ×©×œ×‘ ×¨××©×•×Ÿ: ×‘×§×©×ª ×“×’××™×
        prompt = f"""×‘×”×ª×‘×¡×¡ ×¢×œ ×”×§×¨×™×˜×¨×™×•× ×™×: {json.dumps(answers, ensure_ascii=False)},
×ª×Ÿ ×¨×©×™××ª 5 ×“×’××™ ×¨×›×‘×™× ××ª××™××™× (×™×“ 2 ×‘×™×©×¨××œ). ×”×—×–×¨ JSON:
{{"recommendations":[{{"model":"×“×’×","why":"× ×™××•×§ ×§×¦×¨"}}]}}"""
        txt = call_model(prompt)
        try:
            recs = json.loads(re.search(r"\{.*\}", txt, re.S).group())
        except Exception:
            recs = {"recommendations":[]}

        all_models = [r["model"] for r in recs.get("recommendations",[])]
        if not all_models:
            all_models = ["×˜×•×™×•×˜×” ×§×•×¨×•×œ×”", "×××–×“×” 3", "×§×™×” ×¡×™×“"]

        results = []
        # ×©×œ×‘ ×©× ×™: ×‘×“×™×§×ª ×××™× ×•×ª ×œ×›×œ ×“×’×
        for model in all_models:
            sub_prompt = f"""×‘×“×•×§ ×¢×‘×•×¨ ×”×“×’× {model} (×™×“ ×©× ×™×™×” ×‘×™×©×¨××œ):
- ×¦×™×•×Ÿ ×××™× ×•×ª ×›×œ×œ×™ (0â€“100),
- ×¢×œ×•×™×•×ª ×ª×—×–×•×§×” ×©× ×ª×™×•×ª ×××•×¦×¢×•×ª (×©"×—),
- ×ª×§×œ×•×ª × ×¤×•×¦×•×ª.
×”×—×–×¨ JSON:
{{"model":"{model}","reliability":90,"annual_cost":4500,"issues":["×’×™×¨","×—×©××œ"]}}"""
            sub_txt = call_model(sub_prompt)
            try:
                data = json.loads(re.search(r"\{.*\}", sub_txt, re.S).group())
                results.append(data)
            except Exception:
                results.append({"model":model,"reliability":50,"annual_cost":5000,"issues":["× ×ª×•×Ÿ ×—×¡×¨"]})

        # ×˜×‘×œ×” ××¡×›××ª
        table_md = "| ×“×’× | ×××™× ×•×ª | ×¢×œ×•×ª ×©× ×ª×™×ª | ×ª×§×œ×•×ª × ×¤×•×¦×•×ª |\n|---|---|---|---|\n"
        best_model = None
        best_score = -1
        for r in results:
            score = r.get("reliability",0) - int(r.get("annual_cost",0)/1000)
            if score > best_score:
                best_score = score
                best_model = r["model"]
            table_md += f"| {r['model']} | {r.get('reliability','?')} | {r.get('annual_cost','?')} | {', '.join(r.get('issues',[]))} |\n"

        final_msg = "### ×ª×•×¦××•×ª ×‘×“×™×§×ª ×××™× ×•×ª ×•×ª×—×–×•×§×”\n" + table_md + f"\nâœ… ×”×”××œ×¦×” ×”××•×‘×™×œ×”: **{best_model}**"
        with st.chat_message("assistant"):
            st.markdown(final_msg)
        st.session_state.messages.append({"role":"assistant","content":final_msg})

st.markdown("---")
st.caption("×‘×¡×™×•× ×”×©××œ×•×Ÿ: ×©×œ×‘ ×”××œ×¦×•×ª ×—×›××•×ª ×›×•×œ×œ ×××™× ×•×ª, ×¢×œ×•×™×•×ª ×•×ª×§×œ×•×ª × ×¤×•×¦×•×ª.")
