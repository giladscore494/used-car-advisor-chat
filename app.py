import os
import json
import re
from dataclasses import dataclass
from typing import List, Dict, Any, Optional

import streamlit as st

# Optional SDKs â€“ install according to chosen provider
try:
    from openai import OpenAI  # pip install openai>=1.40.0
except Exception:
    OpenAI = None

try:
    import google.generativeai as genai  # pip install google-generativeai
except Exception:
    genai = None

st.set_page_config(page_title="×™×•×¢×¥ ×¨×›×‘×™× ×™×“ 2 â€“ ×¦'××˜ ×¢× ×©××œ×•×Ÿ ××•×˜××¢", page_icon="ğŸ¤–ğŸš—", layout="centered")

RTL = """
<style>
html, body, [class*="css"] { direction: rtl; text-align: right; }
.block-container { padding-top: .6rem; max-width: 880px; }
.stChatMessage { text-align: right; }
.user-msg { background: #eef6ff; padding: .65rem .9rem; border-radius: 14px; }
.bot-msg { background: #f8fafc; padding: .65rem .9rem; border-radius: 14px; }
.badge { display:inline-block; padding:.15rem .55rem; border-radius:999px; background:#e2e8f0; margin-left:.35rem; font-size:.8rem }
.chip { display:inline-block; padding: .4rem .75rem; border-radius: 999px; border:1px solid #e5e7eb; margin: .25rem; cursor:pointer }
.progress{height:8px;background:#e5e7eb;border-radius:999px;overflow:hidden}
.progress>div{height:100%;background:#22c55e}
hr{border:none;border-top:1px solid #eee;margin:.7rem 0}
</style>
"""
st.markdown(RTL, unsafe_allow_html=True)

# =========================
# Domain: Questionnaire Slots
# =========================
@dataclass
class Slot:
    key: str
    label: str
    prompt: str
    kind: str  # 'select'|'int'|'text'
    required: bool = True
    options: Optional[List[str]] = None

SLOTS: List[Slot] = [
    Slot("budget_min", "×ª×§×¦×™×‘ ××™× ×™××•× (â‚ª)", "××” ×”×ª×§×¦×™×‘ ×”××™× ×™××œ×™ ×©×œ×š ×‘×©×§×œ×™×?", "int"),
    Slot("budget_max", "×ª×§×¦×™×‘ ××§×¡×™××•× (â‚ª)", "×•××” ×”××§×¡×™××•× ×©××ª×” ××•×›×Ÿ ×œ×©×œ×?", "int"),
    Slot("body", "×¡×•×’ ×¨×›×‘", "××™×–×” ×¡×•×’ ×¨×›×‘ ××ª×” ××—×¤×© â€“ ×§×˜×Ÿ / ××©×¤×—×ª×™ / ×’'×™×¤ / ××¡×—×¨×™ ×§×œ?", "select", options=["×§×˜×Ÿ","××©×¤×—×ª×™","×’'×™×¤","××¡×—×¨×™ ×§×œ"]),
    Slot("character", "××•×¤×™ ×¨×›×‘", "×”×¢×“×¤×”: ×¡×¤×•×¨×˜×™×‘×™ ××• ×“×™×™×œ×™ (×™×•××™×•××™)?", "select", options=["×¡×¤×•×¨×˜×™×‘×™","×“×™×™×œ×™"]),
    Slot("usage", "×©×™××•×© ×¢×™×§×¨×™", "×”×©×™××•×© ×”×¢×™×§×¨×™ ×™×”×™×” ×™×•×ª×¨ ×‘×¢×™×¨, ×‘×™×Ÿ-×¢×™×¨×•× ×™ ××• ×©×˜×—?", "select", options=["×¢×™×¨×•× ×™","×‘×™×ŸÖ¾×¢×™×¨×•× ×™","×©×˜×—"]),
    Slot("priority", "×¢×“×™×¤×•×ª", "××” ×”×¢×“×™×¤×•×ª ×”××¨×›×–×™×ª â€“ ×××™× ×•×ª / × ×•×—×•×ª / ×‘×™×¦×•×¢×™× / ×¢×™×¦×•×‘?", "select", options=["×××™× ×•×ª","× ×•×—×•×ª","×‘×™×¦×•×¢×™×","×¢×™×¦×•×‘"]),
    Slot("passengers", "××¡×¤×¨ × ×•×¡×¢×™× ×××•×¦×¢", "×‘×××•×¦×¢ ×›××” × ×•×¡×¢×™× ×™×™×¡×¢×• ×‘×¨×›×‘?", "int"),
    Slot("fuel", "×¡×•×’ ×“×œ×§", "×”×× ×™×© ×¢×“×™×¤×•×ª ×œ×“×œ×§: ×‘× ×–×™×Ÿ / ×“×™×–×œ / ×”×™×‘×¨×™×“×™ / ×—×©××œ×™ ××• ×©×›×œ ××¤×©×¨×•×ª ×¤×ª×•×—×”?", "select", options=["×‘× ×–×™×Ÿ","×“×™×–×œ","×”×™×‘×¨×™×“×™","×—×©××œ×™","×›×œ"]),
    Slot("year_min", "×©× ×ª ×™×™×¦×•×¨ ××™× ×™××œ×™×ª", "×××™×–×• ×©× ×ª ×™×™×¦×•×¨ ××™× ×™××œ×™×ª ×ª×¨×¦×”?", "int"),
    Slot("parking", "×—× ×™×”", "×™×© ×œ×š ×—× ×™×” ×¤×¨×˜×™×ª ××• ×—× ×™×” ×‘×¨×—×•×‘?", "select", options=["×¤×¨×˜×™×ª","×¨×—×•×‘"]),
    Slot("fuel_importance", "×—×©×™×‘×•×ª ×¦×¨×™×›×ª ×“×œ×§", "×›××” ×—×©×•×‘×” ×œ×š ×¦×¨×™×›×ª ×“×œ×§ â€“ ×œ× ×—×©×•×‘ / ×—×©×•×‘ / ×§×¨×™×˜×™?", "select", options=["×œ× ×—×©×•×‘","×—×©×•×‘","×§×¨×™×˜×™"]),
    Slot("km_per_year", '×§"× ×œ×©× ×”', "×›××” ×§×™×œ×•××˜×¨×™× ××ª×” × ×•×¡×¢ ×‘×¢×¨×š ×‘×©× ×”?", "int"),
    Slot("tax_importance", "×—×©×™×‘×•×ª ××’×¨×ª ×˜×¡×˜", "×¢×“ ×›××” ×—×©×•×‘×” ×¢×œ×•×ª ××’×¨×ª ×”×¨×™×©×•×™ â€“ ×œ× ×—×©×•×‘ / ×—×©×•×‘ / ×§×¨×™×˜×™?", "select", options=["×œ× ×—×©×•×‘","×—×©×•×‘","×§×¨×™×˜×™"]),
    Slot("ins_importance", "×—×©×™×‘×•×ª ×‘×™×˜×•×—", "×¢×“ ×›××” ×—×©×•×‘×” ×¢×œ×•×ª ×”×‘×™×˜×•×— â€“ ×œ× ×—×©×•×‘ / ×—×©×•×‘ / ×§×¨×™×˜×™?", "select", options=["×œ× ×—×©×•×‘","×—×©×•×‘","×§×¨×™×˜×™"]),
    Slot("gearbox", "×ª×™×‘×ª ×”×™×œ×•×›×™× (×œ× ×—×•×‘×”)", "×™×© ×œ×š ×”×¢×“×¤×” ×œ×’×™×¨ â€“ ××•×˜×•××˜ ××• ×™×“× ×™?", "select", required=False, options=["×œ× ××©× ×”","××•×˜×•××˜","×™×“× ×™"]),
    Slot("region", "××–×•×¨ ×‘××¨×¥ (×œ× ×—×•×‘×”)", "×‘××™×–×• ×¢×™×¨/××–×•×¨ ×‘××¨×¥ ××ª×” ×’×¨?", "text", required=False),
]
REQUIRED_KEYS = [s.key for s in SLOTS if s.required]

# =========================
# App State
# =========================
if "messages" not in st.session_state:
    st.session_state.messages: List[Dict[str, str]] = [
        {"role":"assistant","content":"×”×™×™! ×× ×™ ×”×™×•×¢×¥ ×œ×¨×›×‘×™× ×™×“ 2. × ×ª×—×™×œ ×‘×©××œ×” ×§×¦×¨×” â€“ ××” ×˜×•×•×— ×”×ª×§×¦×™×‘ ×©×œ×š? (××¤×©×¨ ×œ×›×ª×•×‘ ×—×•×¤×©×™ ××• ×œ×‘×—×•×¨ ×‘×¤×ª×§×™×•×ª ×›××Ÿ ×œ××˜×”)"}
    ]
if "answers" not in st.session_state:
    st.session_state.answers: Dict[str, Any] = {}
if "ask_index" not in st.session_state:
    st.session_state.ask_index = 0
if "last_ask" not in st.session_state:
    st.session_state.last_ask = None
if "_clicked_choice" not in st.session_state:
    st.session_state._clicked_choice = None

# =========================
# Providers & status
# =========================
PROVIDER = st.sidebar.selectbox("×¡×¤×§ ××•×“×œ", ["OpenAI", "Gemini"], index=0)

openai_key = os.getenv("OPENAI_API_KEY", "")
gemini_key = os.getenv("GEMINI_API_KEY", "") or os.getenv("GOOGLE_API_KEY", "")  # accept both names

if PROVIDER == "OpenAI":
    has_key = bool(openai_key and OpenAI)
    model_name = st.sidebar.text_input("OpenAI Model", value="gpt-4.1-mini")
    oai_client = OpenAI(api_key=openai_key) if has_key else None
else:
    has_key = bool(gemini_key and genai)
    model_name = st.sidebar.text_input("Gemini Model", value="gemini-1.5-flash")
    if has_key:
        genai.configure(api_key=gemini_key)
        gem_model = genai.GenerativeModel(model_name)
    else:
        gem_model = None

st.sidebar.markdown(f"**×¡×˜×˜×•×¡ ×¡×¤×§:** {'âœ… ××—×•×‘×¨' if has_key else 'âŒ ×œ×œ× ××¤×ª×—/×¡×¤×¨×™×”'}")

# =========================
# Small helpers (normalization & coercion)
# =========================
def _norm(s: str) -> str:
    s = s or ""
    s = s.replace("×´", '"').replace("â€", '"').replace("â€œ", '"').replace("×³","'").replace("â€™","'").replace("Ö¾","-")
    s = s.replace("×’×³×™×¤", "×’'×™×¤")
    return re.sub(r"\s+", " ", s.strip())

def coerce_for_slot(slot: Slot, text: str) -> Any:
    t = _norm(text)
    if slot.kind == "int":
        nums = re.findall(r"\d+", t.replace(",", ""))
        if nums:
            try:
                return int(nums[0])
            except Exception:
                return None
        return None
    if slot.kind == "select" and slot.options:
        tn = t.lower()
        if slot.key == "fuel" and "×‘× ×–×™×Ÿ" in tn:
            return "×‘× ×–×™×Ÿ"
        for opt in slot.options:
            if _norm(opt).lower() in tn or tn in _norm(opt).lower():
                return opt
        for opt in slot.options:
            if _norm(opt)[0] == t[:1]:
                return opt
        return t
    return t if t else None

def parse_budget_range(text: str) -> Optional[Dict[str,int]]:
    t = text.replace(",", "")
    nums = [int(n) for n in re.findall(r"\d+", t)]
    if len(nums) >= 2:
        lo, hi = sorted(nums[:2])
        return {"budget_min": lo, "budget_max": hi}
    return None

def next_missing_required() -> Optional[Slot]:
    for s in SLOTS:
        if s.required and (s.key not in st.session_state.answers or st.session_state.answers[s.key] in [None,"",0]):
            return s
    return None

# =========================
# LLM call
# =========================
SYSTEM_PROMPT = {
    "role":"system",
    "content":(
        "××ª×” ×¢×•×–×¨ ×§× ×™×•×ª ×¨×›×‘ ×™×“ 2 ×‘×¢×‘×¨×™×ª. ×ª×©××œ ×©××œ×” ××—×ª ×§×¦×¨×” ×‘×›×œ ×¤×¢×, ×”×¦×¢ 3â€“6 ××¤×©×¨×•×™×•×ª ×§×¦×¨×•×ª ×›×›×¤×ª×•×¨×™ ×‘×—×™×¨×”,"
        " ×•×’× ×§×‘×œ ×ª×©×•×‘×” ×—×•×¤×©×™×ª. ×¢×“×›×Ÿ ××™×œ×•×Ÿ JSON ×‘×©× filled_slots ×›×©× ××¡×¨×™× ×¢×¨×›×™×, ×•×‘×—×¨ ××ª ×”×©×“×” ×”×‘× ×©× ×“×¨×©."
        " ×ª××™×“ ×”×—×–×¨ ×‘×¤×•×¨××˜ JSON ×ª×§×™×Ÿ:")
}
FORMAT_HINT = {
    "role":"system",
    "content":(
        '{"assistant_message": "×˜×§×¡×˜ ×œ×‘×Ÿ-××“×",'
        ' "filled_slots": {"budget_min": 40000, "body": "××©×¤×—×ª×™"},'
        ' "ask_next": {"key": "fuel", "question": "××™×–×” ×¡×•×’ ×“×œ×§ ×ª×¢×“×™×£?", "options": ["×‘× ×–×™×Ÿ","×“×™×–×œ","×”×™×‘×¨×™×“×™","×—×©××œ×™","×›×œ"]}}')
}

def build_chat_for_llm(messages: List[Dict[str,str]], answers: Dict[str,Any]) -> List[Dict[str,str]]:
    ctx = [SYSTEM_PROMPT, FORMAT_HINT]
    state_line = json.dumps({k: v for k, v in answers.items()}, ensure_ascii=False)
    ctx.append({"role":"system", "content": f"××¦×‘ ×©×“×•×ª × ×•×›×—×™: {state_line}"})
    ctx.extend(messages[-8:])
    return ctx

def call_llm(context_msgs: List[Dict[str,str]]) -> Dict[str,Any]:
    try:
        if PROVIDER == "OpenAI" and has_key and oai_client:
            resp = oai_client.chat.completions.create(
                model=model_name,
                messages=context_msgs,
                temperature=0.3,
            )
            txt = resp.choices[0].message.content
        elif PROVIDER == "Gemini" and has_key and gem_model:
            as_text = "\n".join([f"{m['role']}: {m['content']}" for m in context_msgs])
            r = gem_model.generate_content(as_text)
            txt = r.text or ""
        else:
            txt = '{"assistant_message": "(××™×Ÿ ×—×™×‘×•×¨ ×œ×¡×¤×§ â€“ ××¦×‘ ××§×•××™)", "filled_slots": {}, "ask_next": null}'
    except Exception:
        txt = '{"assistant_message": "(×©×’×™××” ××¦×œ ×”×¡×¤×§ â€“ ××¦×‘ ××§×•××™)", "filled_slots": {}, "ask_next": null}'

    try:
        m = re.search(r"\{[\s\S]*\}$", txt.strip())
        payload = json.loads(m.group(0) if m else txt)
        if not isinstance(payload, dict):
            raise ValueError("not a dict")
        return payload
    except Exception:
        return {"assistant_message":"", "filled_slots":{}, "ask_next":None}

# =========================
# UI helpers
# =========================
def render_quick_replies(options: List[str]):
    if not options:
        return None
    cols = st.columns(min(3, len(options)))
    clicks = None
    for i, opt in enumerate(options):
        with cols[i % len(cols)]:
            if st.button(opt, key=f"qr-{opt}-{i}"):
                clicks = opt
    return clicks

def progress_bar(answers: Dict[str,Any]):
    filled = sum(1 for k in REQUIRED_KEYS if k in answers and answers[k] not in [None, "", 0])
    pct = int(100 * filled / max(1, len(REQUIRED_KEYS)))
    st.markdown(f"**×”×ª×§×“××•×ª ×”×©××œ×•×Ÿ:** {pct}%")
    st.markdown(f"<div class='progress'><div style='width:{pct}%'></div></div>", unsafe_allow_html=True)

# =========================
# Display history
# =========================
st.markdown("## ğŸ¤– ×™×•×¢×¥ ×¨×›×‘×™× â€“ ×¦'××˜ ×¢× ×©××œ×•×Ÿ ××•×‘× ×”")
progress_bar(st.session_state.answers)

for m in st.session_state.messages:
    with st.chat_message("assistant" if m["role"]=="assistant" else "user"):
        st.markdown(m["content"])

# =========================
# In-chat questionnaire turn
# =========================
user_text = st.chat_input("×›×ª×•×‘ ×ª×©×•×‘×” ×—×•×¤×©×™×ªâ€¦ ××• ×‘×—×¨ ××¤×©×¨×•×ª ×›××©×¨ ×ª×•×¦×’ ××¢×œ ××™× ×¤×•×˜ ×–×”")

# Handle quick-reply clicks BEFORE sending anything to the model
clicked_choice = st.session_state.get("_clicked_choice")
if clicked_choice:
    last_ask = st.session_state.get("last_ask")
    if last_ask and isinstance(last_ask, dict) and last_ask.get("key"):
        st.session_state.answers[last_ask["key"]] = clicked_choice
        st.session_state.messages.append({"role":"user","content":clicked_choice})
        st.session_state._clicked_choice = None
        st.session_state.last_ask = None
        user_text = None
    else:
        user_text = clicked_choice
        st.session_state._clicked_choice = None

if user_text:
    st.session_state.messages.append({"role":"user","content":user_text})

    # Bind free-text to the last asked slot in ALL modes (prevents loops)
    if st.session_state.get("last_ask") and st.session_state.last_ask.get("key"):
        key = st.session_state.last_ask["key"]
        slot = next((s for s in SLOTS if s.key == key), None)
        if slot:
            val = coerce_for_slot(slot, user_text or "")
            if val not in [None, ""]:
                st.session_state.answers[key] = val
        st.session_state.last_ask = None

    # Try infer budget range from any free text
    rng = parse_budget_range(user_text or "")
    if rng:
        st.session_state.answers.setdefault("budget_min", rng["budget_min"])
        st.session_state.answers.setdefault("budget_max", rng["budget_max"])

    # LLM path (if connected), else local logic
    if has_key:
        ctx = build_chat_for_llm(st.session_state.messages, st.session_state.answers)
        payload = call_llm(ctx)

        filled_slots = payload.get("filled_slots") or {}
        if isinstance(filled_slots, dict):
            for k, v in list(filled_slots.items()):
                if k in ["budget_min","budget_max","passengers","km_per_year","year_min"]:
                    try:
                        filled_slots[k] = int(str(v).replace(",",""))
                    except Exception:
                        pass
            st.session_state.answers.update({k: v for k, v in filled_slots.items() if v not in [None, ""]})

        assistant_message = payload.get("assistant_message") or "×§×™×‘×œ×ª×™. × ××©×™×š."

        ask_next = payload.get("ask_next")
        # Guard: if model asks again for a field already filled, skip to next missing
        if ask_next and isinstance(ask_next, dict):
            next_key = ask_next.get("key")
            if next_key and next_key in st.session_state.answers and st.session_state.answers[next_key] not in [None,"",0]:
                ask_next = None

        if ask_next and isinstance(ask_next, dict):
            q = ask_next.get("question") or "×©××œ×” ×”×‘××”:"
            opts = ask_next.get("options", [])
            st.session_state.last_ask = {"key": ask_next.get("key"), "options": opts}
            with st.chat_message("assistant"):
                st.markdown(assistant_message + f"\n\n**{q}**")
                choice = render_quick_replies(opts)
                if choice:
                    st.session_state._clicked_choice = choice
            st.session_state.messages.append({"role":"assistant","content":assistant_message + ("\n\n" + q if q else "")})
        else:
            missing = [s for s in SLOTS if s.required and s.key not in st.session_state.answers]
            if missing:
                nxt = missing[0]
                st.session_state.last_ask = {"key": nxt.key, "options": (nxt.options or [])}
                with st.chat_message("assistant"):
                    st.markdown(assistant_message + f"\n\n**{nxt.prompt}**")
                    choice = render_quick_replies(nxt.options or [])
                    if choice:
                        st.session_state._clicked_choice = choice
                st.session_state.messages.append({"role":"assistant","content":assistant_message + "\n\n" + nxt.prompt})
            else:
                shortlist = [
                    "×˜×•×™×•×˜×” ×§×•×¨×•×œ×” ×”×™×‘×¨×™×“×™×ª",
                    "×××–×“×” 3",
                    "×§×™×” ×¡×¤×•×¨×˜××–'",
                    "×™×•× ×“××™ ×˜×•×¡×•×Ÿ",
                    "×˜×•×™×•×˜×” ×™××¨×™×¡ ×”×™×‘×¨×™×“×™×ª",
                ]
                bullet = "\n".join([f"â€¢ {x}" for x in shortlist])
                summ = (
                    "×¡×™×™×× ×• ×œ××¡×•×£ × ×ª×•× ×™×! ×”× ×” 5 ×“×’××™× ××ª××™××™× ×œ×”×ª×—×œ×” (MVP):\n\n" + bullet +
                    "\n\n×ª×¨×¦×” ×œ×¢×‘×•×¨ ×œ×©×œ×‘ '××•×“×¢×” ×¡×¤×¦×™×¤×™×ª' ×›×“×™ ×œ×—×©×‘ ×¦×™×•×Ÿ ×›×“××™×•×ª 0â€“100?"
                )
                with st.chat_message("assistant"):
                    st.markdown(summ)
                st.session_state.messages.append({"role":"assistant","content":summ})
    else:
        # Local (no provider) â€“ ask next missing or finish
        missing = [s for s in SLOTS if s.required and s.key not in st.session_state.answers]
        if missing:
            nxt = missing[0]
            st.session_state.last_ask = {"key": nxt.key, "options": (nxt.options or [])}
            with st.chat_message("assistant"):
                st.markdown("×ª×•×“×”!\n\n**" + nxt.prompt + "**")
                choice = render_quick_replies(nxt.options or [])
                if choice:
                    st.session_state._clicked_choice = choice
            st.session_state.messages.append({"role":"assistant","content":"×ª×•×“×”!\n\n" + nxt.prompt})
        else:
            shortlist = [
                "×˜×•×™×•×˜×” ×§×•×¨×•×œ×” ×”×™×‘×¨×™×“×™×ª",
                "×××–×“×” 3",
                "×§×™×” ×¡×¤×•×¨×˜××–'",
                "×™×•× ×“××™ ×˜×•×¡×•×Ÿ",
                "×˜×•×™×•×˜×” ×™××¨×™×¡ ×”×™×‘×¨×™×“×™×ª",
            ]
            bullet = "\n".join([f"â€¢ {x}" for x in shortlist])
            summ = (
                "×¡×™×™×× ×• ×œ××¡×•×£ × ×ª×•× ×™×! ×”× ×” 5 ×“×’××™× ××ª××™××™× ×œ×”×ª×—×œ×” (MVP):\n\n" + bullet +
                "\n\n×ª×¨×¦×” ×œ×¢×‘×•×¨ ×œ×©×œ×‘ '××•×“×¢×” ×¡×¤×¦×™×¤×™×ª' ×›×“×™ ×œ×—×©×‘ ×¦×™×•×Ÿ ×›×“××™×•×ª 0â€“100?"
            )
            with st.chat_message("assistant"):
                st.markdown(summ)
            st.session_state.messages.append({"role":"assistant","content":summ})

# =========================
# Footer / Next steps
# =========================
st.markdown("---")
st.caption("""×’×¨×¡×ª MVP: ×©××œ×•×Ÿ ××•×˜××¢ ×‘×¦'××˜ + ×‘×—×™×¨×ª ×¡×¤×§ ××•×“×œ (OpenAI/Gemini).
×‘×”××©×š: × ×™×§×•×“ ××ª×§×“×, ×‘×“×™×§×ª ×××™× ×•×ª, ×•×™×¦×•× PDF.""")
