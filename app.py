import os
import re
import time
import json
import requests
import streamlit as st
import pandas as pd
from openai import OpenAI

# =============================
# ××¤×ª×—×•×ª API
# =============================
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY")

if not OPENAI_API_KEY or not PERPLEXITY_API_KEY:
    st.error("âŒ ×œ× × ××¦××• ××¤×ª×—×•×ª API. ×•×“× ×©×”×’×“×¨×ª ××•×ª× ×‘-secrets.")
    st.stop()

client = OpenAI(api_key=OPENAI_API_KEY)

# =============================
# Cache ×¤× ×™××™ (24 ×©×¢×•×ª)
# =============================
cache = {}  # { hash_key: (timestamp, result) }

def make_key(answers):
    return f"{answers['budget_min']}-{answers['budget_max']}-{answers['engine']}-{answers['usage']}-{answers['size']}-{answers['car_type']}-{answers['turbo']}-{answers['gearbox']}-{answers['engine_size']}-{answers['year_range']}"

def get_from_cache(answers, max_age_hours=24):
    key = make_key(answers)
    if key in cache:
        ts, result = cache[key]
        if time.time() - ts < max_age_hours * 3600:
            return result
    return None

def save_to_cache(answers, result):
    key = make_key(answers)
    cache[key] = (time.time(), result)

# =============================
# ×§×¨×™××” ×‘×˜×•×—×” ×œ-Perplexity
# =============================
def safe_perplexity_call(payload):
    url = "https://api.perplexity.ai/chat/completions"
    headers = {"Authorization": f"Bearer {PERPLEXITY_API_KEY}", "Content-Type": "application/json"}
    try:
        r = requests.post(url, headers=headers, json=payload, timeout=60)
        data = r.json()
        if "choices" not in data:
            return f"×©×’×™××ª API: {data}"
        return data["choices"][0]["message"]["content"]
    except Exception as e:
        return f"×©×’×™××”: {e}"

def extract_numbers_from_text(text):
    numbers = re.findall(r'\d{1,3}(?:[ ,]\d{3})*|\d+', text)
    return [int(n.replace(",", "").replace(" ", "")) for n in numbers]

# =============================
# ×©×œ×‘ 1 â€“ GPT ××¦×™×¢ ×“×’××™× ×¨××©×•× ×™×™×
# =============================
def analyze_needs_with_gpt(answers):
    prompt = f"""
    ×”××©×ª××© × ×ª×Ÿ ××ª ×”×”×¢×“×¤×•×ª:
    {answers}

    ×”×—×–×¨ ×¨×©×™××” ×©×œ 5â€“7 ×“×’××™ ×¨×›×‘×™× ××ª××™××™×.
    ×“×¨×™×©×•×ª ×—×•×‘×”:
    - ×¨×§ ×“×’××™× ×©× ××›×¨×™× ×‘×™×©×¨××œ ×‘×™×“ ×©× ×™×™×”.
    - ×¨×§ ×¨×›×‘×™× ×©×”××—×™×¨×•×Ÿ ×©×œ×”× ×‘×™×“ ×©× ×™×™×” × ××¦× ×‘×˜×•×•×— {answers['budget_min']}â€“{answers['budget_max']} â‚ª.
    - ×× ×•×¢ {answers['engine']}, × ×¤×— {answers['engine_size']} ×¡××´×§.
    - ×©× ×•×ª ×™×™×¦×•×¨: {answers['year_range']}.
    - ×¡×•×’ ×¨×›×‘ ××•×¢×“×£: {answers['car_type']}
    - ×”×¢×“×¤×ª ×ª×™×‘×ª ×”×™×œ×•×›×™×: {answers['gearbox']}
    - ×”×¢×“×¤×ª ×˜×•×¨×‘×•: {answers['turbo']}
    """
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2,
    )
    text = response.choices[0].message.content
    clean_models = [re.sub(r"^[0-9\.\-\â€¢\*\s]+", "", line.strip()) for line in text.split("\n") if line.strip()]
    return clean_models

# =============================
# ×©×œ×‘ 2 â€“ ×¡×™× ×•×Ÿ ×¢× Perplexity
# =============================
def filter_models_for_israel(models, min_budget, max_budget, engine, gearbox, turbo, engine_size, year_range):
    filtered, debug_info = [], {}
    for model_name in models:
        payload = {
            "model": "sonar",
            "messages": [
                {"role": "system", "content": "×”×—×–×¨ ×ª×©×•×‘×” ×‘×¢×‘×¨×™×ª ×•×‘×§×¦×¨×” ×‘×œ×‘×“."},
                {"role": "user", "content": f"×”×× {model_name} × ××›×¨ ×‘×™×©×¨××œ ×‘×™×“ ×©× ×™×™×” ×¢× ×× ×•×¢ {engine} {engine_size} ×¡×\"×§, ×’×™×¨ {gearbox}, {'×˜×•×¨×‘×•' if turbo=='×›×Ÿ' else '×œ×œ× ×˜×•×¨×‘×•'}, ×©× ×•×ª ×™×™×¦×•×¨ {year_range}? ×•××” ×˜×•×•×— ×”××—×™×¨×™× ×”×××™×ª×™ ×©×œ×•?"}
            ]
        }
        answer = safe_perplexity_call(payload)
        debug_info[model_name] = answer

        if isinstance(answer, str):
            ans = answer.lower()
            if "×œ× × ×¤×•×¥" in ans or "×œ× × ××›×¨" in ans:
                continue
            nums = extract_numbers_from_text(answer)
            if len(nums) >= 2:
                low, high = min(nums), max(nums)
                if low <= max_budget and high >= min_budget:
                    filtered.append(model_name)
            else:
                if "× ×¤×•×¥" in ans or "× ××›×¨" in ans:
                    filtered.append(model_name)
    return filtered, debug_info

# =============================
# ×©×œ×‘ 3 â€“ × ×ª×•× ×™× ××œ××™× ×‘-JSON
# =============================
def fetch_models_data_with_perplexity(models, answers):
    all_data = {}
    for model_name in models:
        payload = {
            "model": "sonar-pro",
            "messages": [
                {"role": "system", "content": "×¢× ×” ×‘×¤×•×¨××˜ JSON ×‘×œ×‘×“. ××œ ×ª×•×¡×™×£ ×˜×§×¡×˜ ×—×•×¤×©×™."},
                {"role": "user", "content": f"""
                ×”×‘× ××™×“×¢ ×¢×“×›× ×™ ×¢×œ {model_name} ×‘×™×©×¨××œ.
                ×”×—×–×¨ ×ª×©×•×‘×” ×‘×¤×•×¨××˜ JSON ×¢× ×”×©×“×•×ª ×”×‘××™×:
                {{
                 "price_range": "×˜×•×•×— ××—×™×¨×•×Ÿ ×××•×¦×¢ ×‘×™×“ ×©× ×™×™×”",
                 "availability": "×–××™× ×•×ª ×•× ×¤×•×¦×•×ª ×‘×™×©×¨××œ",
                 "insurance": "×¢×œ×•×ª ×‘×™×˜×•×— ×××•×¦×¢×ª",
                 "license_fee": "××’×¨×ª ×¨×™×©×•×™/×˜×¡×˜ ×©× ×ª×™×ª",
                 "maintenance": "×ª×—×–×•×§×” ×©× ×ª×™×ª ×××•×¦×¢×ª",
                 "common_issues": "×ª×§×œ×•×ª × ×¤×•×¦×•×ª ×™×“×•×¢×•×ª",
                 "fuel_consumption": "×¦×¨×™×›×ª ×“×œ×§ ×××™×ª×™×ª",
                 "depreciation": "×™×¨×™×“×ª ×¢×¨×š ×××•×¦×¢×ª",
                 "safety": "×“×™×¨×•×’ ×‘×˜×™×—×•×ª",
                 "parts_availability": "×–××™× ×•×ª ×—×œ×¤×™× ×‘×™×©×¨××œ"
                }}
                """}
            ]
        }
        answer = safe_perplexity_call(payload)
        try:
            parsed = json.loads(answer)
        except:
            parsed = {"price_range": answer}  # fallback
        all_data[model_name] = parsed
    return all_data

# =============================
# ×©×œ×‘ 4 â€“ GPT ××¡×›× ×”××œ×¦×”
# =============================
def final_recommendation_with_gpt(answers, models, models_data):
    text = f"""
    ×ª×©×•×‘×•×ª ×”××©×ª××©:
    {answers}

    ×“×’××™× ×–××™× ×™× ×‘×™×©×¨××œ:
    {models}

    × ×ª×•× ×™ Perplexity (JSON):
    {models_data}

    ×¦×•×¨ ×”××œ×¦×” ×¡×•×¤×™×ª ×‘×¢×‘×¨×™×ª:
    - ×”×¦×’ ×¢×“ 5 ×“×’××™× ×‘×œ×‘×“
    - ×¤×¨×˜ ×™×ª×¨×•× ×•×ª ×•×—×¡×¨×•× ×•×ª
    - ×›×œ×•×œ × ×™××•×§×™× ××™×©×™×™× ×œ×¤×™ ×”×ª×§×¦×™×‘, ×¡×•×’ ×”×× ×•×¢, × ×¤×— ×× ×•×¢, ×©× ×•×ª ×™×™×¦×•×¨, ×˜×•×¨×‘×• ×•×’×™×¨
    """
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": text}],
        temperature=0.5,
    )
    return response.choices[0].message.content

# =============================
# Streamlit UI
# =============================
st.set_page_config(page_title="Car-Advisor", page_icon="ğŸš—")
st.title("ğŸš— Car-Advisor â€“ ×™×•×¢×¥ ×¨×›×‘×™× ×—×›×")

with st.form("car_form"):
    st.write("×¢× ×” ×¢×œ ×”×©××œ×•×Ÿ:")

    answers = {}
    answers["budget_range"] = st.selectbox("××” ×˜×•×•×— ×”×ª×§×¦×™×‘ ×©×œ×š ×œ×¨×›×‘?", ["5â€“10K", "10â€“20K", "20â€“40K", "40K+"])
    answers["budget_min"] = int(st.text_input("×ª×§×¦×™×‘ ××™× ×™××œ×™ (â‚ª)", "10000"))
    answers["budget_max"] = int(st.text_input("×ª×§×¦×™×‘ ××§×¡×™××œ×™ (â‚ª)", "20000"))
    answers["km"] = st.selectbox("×›××” ×§×™×œ×•××˜×¨×™× ××ª×” × ×•×¡×¢ ×‘×—×•×“×©?", ["<1000", "1000â€“2000", "2000â€“4000", "4000+"])
    answers["engine"] = st.radio("××™×–×” ×¡×•×’ ×× ×•×¢ ××ª×” ××¢×“×™×£?", ["×‘× ×–×™×Ÿ", "×“×™×–×œ", "×”×™×‘×¨×™×“×™", "×—×©××œ×™"])
    answers["engine_size"] = st.selectbox("××” × ×¤×— ×”×× ×•×¢ ×”××•×¢×“×£?", ["1200", "1600", "2000", "3000+"])
    answers["year_range"] = st.selectbox("××” ×©× ×•×ª ×”×™×™×¦×•×¨ ×”×¨×¦×•×™×•×ª?", ["2010â€“2015", "2016â€“2020", "2021+"])
    answers["usage"] = st.radio("××” ×”×©×™××•×© ×”×¢×™×§×¨×™ ×‘×¨×›×‘?", ["×¢×™×¨×•× ×™", "×‘×™×Ÿ-×¢×™×¨×•× ×™", "××¢×•×¨×‘"])
    answers["size"] = st.selectbox("××™×–×” ×’×•×“×œ ×¨×›×‘ ××ª××™× ×œ×š?", ["×§×˜×Ÿ", "××©×¤×—×ª×™", "SUV", "×˜× ×“×¨"])
    answers["car_type"] = st.selectbox("××™×–×” ×¡×•×’ ×¨×›×‘ ××ª××™× ×œ×š?", ["×¡×“××Ÿ", "×”××¦'×‘×§", "SUV", "×˜× ×“×¨", "××©×¤×—×ª×™"])
    answers["turbo"] = st.radio("××ª×” ××¢×“×™×£ ×× ×•×¢ ×¢× ×˜×•×¨×‘×•?", ["×œ× ××©× ×”", "×›×Ÿ", "×œ×"])
    answers["gearbox"] = st.radio("××™×–×” ×¡×•×’ ×ª×™×‘×ª ×”×™×œ×•×›×™× ××ª×” ××¢×“×™×£?", ["×œ× ××©× ×”", "××•×˜×•××˜", "×™×“× ×™", "×¨×•×‘×•×˜×™"])
    answers["passengers"] = st.radio("×›××” ×× ×©×™× × ×•×¡×¢×™× ×œ×¨×•×‘ ×‘×¨×›×‘?", ["1", "2â€“3", "4â€“5", "6+"])
    answers["fuel_eff"] = st.radio("×¢×“ ×›××” ×—×©×•×‘ ×—×¡×›×•×Ÿ ×‘×“×œ×§?", ["×œ× ×—×©×•×‘", "×‘×™× ×•× ×™", "×—×©×•×‘ ×××•×“"])
    answers["safety"] = st.radio("×¢×“ ×›××” ×—×©×•×‘×” ×¨××ª ×‘×˜×™×—×•×ª?", ["× ××•×š", "×‘×™× ×•× ×™", "×’×‘×•×” ×××•×“"])
    answers["extra"] = st.text_area("×™×© ××©×”×• × ×•×¡×£ ×©×—×©×•×‘ ×œ×¦×™×™×Ÿ?")

    submitted = st.form_submit_button("×©×œ×— ×•×§×‘×œ ×”××œ×¦×”")

if submitted:
    cached_result = get_from_cache(answers)
    if cached_result:
        st.success("âœ… ×”×ª×•×¦××” × ×˜×¢× ×” ××”×××’×¨ (Cache, 24 ×©×¢×•×ª)")
        summary = cached_result
    else:
        with st.spinner("ğŸ¤– GPT ×‘×•×—×¨ ×“×’××™× ×¨××©×•× ×™×™×..."):
            initial_models = analyze_needs_with_gpt(answers)
        st.info(f"ğŸ“‹ ×“×’××™× ×¨××©×•× ×™×™×: {initial_models}")

        with st.spinner("ğŸ‡®ğŸ‡± ××¡× ×Ÿ ×“×’××™× ××•×œ ××—×™×¨×•×Ÿ ×××™×ª×™ ×•×¡×•×’ ×× ×•×¢..."):
            israeli_models, debug_info = filter_models_for_israel(
                initial_models, answers["budget_min"], answers["budget_max"],
                answers["engine"], answers["gearbox"], answers["turbo"],
                answers["engine_size"], answers["year_range"]
            )

        with st.expander("ğŸ” ×ª×©×•×‘×•×ª Perplexity ×œ×¡×™× ×•×Ÿ"):
            st.write(debug_info)

        if not israeli_models:
            st.error("âŒ ×œ× × ××¦××• ×“×’××™× ×–××™× ×™× ×‘×™×©×¨××œ ×‘×”×ª×× ×œ×“×¨×™×©×•×ª.")
        else:
            st.success(f"âœ… ×“×’××™× ×–××™× ×™× ×‘×™×©×¨××œ: {israeli_models}")

            with st.spinner("ğŸŒ ×©×•×œ×£ × ×ª×•× ×™× ××œ××™× ×Ö¾Perplexity..."):
                models_data = fetch_models_data_with_perplexity(israeli_models, answers)

            # ×˜×‘×œ×ª ×”×©×•×•××”
            df = pd.DataFrame(models_data).T
            st.subheader("ğŸ“Š ×”×©×•×•××ª × ×ª×•× ×™× ×‘×™×Ÿ ×”×“×’××™×")
            st.dataframe(df)

            with st.spinner("âš¡ ×™×•×¦×¨ ×”××œ×¦×” ×¡×•×¤×™×ª ×¢× GPT..."):
                summary = final_recommendation_with_gpt(answers, israeli_models, models_data)

            st.subheader("ğŸ” ×”×”××œ×¦×” ×”×¡×•×¤×™×ª ×©×œ×š")
            st.write(summary)

            save_to_cache(answers, summary)

    # ×”×¢×¨×•×ª ×—×©×•×‘×•×ª ×‘×¡×•×£
    st.markdown("---")
    st.markdown("âš ï¸ **×—×©×•×‘ ×œ×“×¢×ª:**")
    st.markdown("1. ××•××œ×¥ ×œ×‘×“×•×§ ××ª [×××’×¨ ×”×¢×‘×¨ ×”×‘×™×˜×•×—×™ ×©×œ ××¨×›×– ×”×¡×œ×™×§×”](https://www.cbc.org.il/) ×œ×§×‘×œ×ª ×”×™×¡×˜×•×¨×™×™×ª ×ª××•× ×•×ª ×¢×œ ×”×¨×›×‘.")
    st.markdown("2. ×¨×¦×•×™ ×œ×§×—×ª ××ª ×”×¨×›×‘ ×œ×‘×“×™×§×” ×‘××›×•×Ÿ ×‘×“×™×§×” ××•×¨×©×” ×œ×¤× ×™ ×¨×›×™×©×”.")
