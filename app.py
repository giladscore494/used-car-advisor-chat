import os
import re
import time
import json
import requests
import streamlit as st
import pandas as pd
from openai import OpenAI

# =============================
# ××¤×ª×—×•×ª API
# =============================
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY")

if not OPENAI_API_KEY or not PERPLEXITY_API_KEY:
    st.error("âŒ ×œ× × ××¦××• ××¤×ª×—×•×ª API. ×•×“× ×©×”×’×“×¨×ª ××•×ª× ×‘-secrets.")
    st.stop()

client = OpenAI(api_key=OPENAI_API_KEY)

# =============================
# Cache ×¤× ×™××™ (24 ×©×¢×•×ª)
# =============================
cache = {}  # { hash_key: (timestamp, result) }

def make_key(answers):
    return f"{answers['budget_min']}-{answers['budget_max']}-{answers['engine']}-{answers['usage']}-{answers['size']}-{answers['car_type']}-{answers['turbo']}-{answers['gearbox']}-{answers['engine_size']}-{answers['year_range']}"

def get_from_cache(answers, max_age_hours=24):
    key = make_key(answers)
    if key in cache:
        ts, result = cache[key]
        if time.time() - ts < max_age_hours * 3600:
            return result
    return None

def save_to_cache(answers, result):
    key = make_key(answers)
    cache[key] = (time.time(), result)

# =============================
# ×§×¨×™××” ×‘×˜×•×—×” ×œ-Perplexity
# =============================
def safe_perplexity_call(payload):
    url = "https://api.perplexity.ai/chat/completions"
    headers = {"Authorization": f"Bearer {PERPLEXITY_API_KEY}", "Content-Type": "application/json"}
    try:
        r = requests.post(url, headers=headers, json=payload, timeout=90)
        data = r.json()
        if "choices" not in data:
            return f"×©×’×™××ª API: {data}"
        return data["choices"][0]["message"]["content"]
    except Exception as e:
        return f"×©×’×™××”: {e}"

# =============================
# ×©×œ×‘ 1 â€“ GPT ××¦×™×¢ ×“×’××™× ×¨××©×•× ×™×™×
# =============================
def analyze_needs_with_gpt(answers):
    prompt = f"""
    {answers}

    ×”×—×–×¨ ×¨×©×™××” × ×§×™×™×” ×©×œ 5â€“7 ×“×’××™ ×¨×›×‘×™× (×©× ×”×“×’× ×‘×œ×‘×“).
    ××œ ×ª×•×¡×™×£ ××¤×¨×˜×™× (×× ×•×¢/×©× ×”/××—×™×¨).
    """
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2,
    )
    text = response.choices[0].message.content
    clean_models = []
    for line in text.split("\n"):
        line = line.strip()
        if not line: 
            continue
        if ":" in line or "×× ×•×¢" in line or "××—×™×¨" in line or "×©× ×”" in line:
            continue
        if line.startswith("×‘×”×ª×‘×¡×¡") or line.startswith("×× ×"):
            continue
        line = re.sub(r"^[0-9\.\-\â€¢\*\s]+", "", line)
        if line:
            clean_models.append(line)
    return clean_models

# =============================
# ×©×œ×‘ 2 â€“ ×¡×™× ×•×Ÿ ×‘×¡×™×¡×™ ××•×œ ×™×©×¨××œ
# =============================
def filter_models_for_israel(models, min_budget, max_budget, engine, gearbox, turbo, engine_size, year_range):
    filtered, debug_info = [], {}
    models_str = ", ".join(models)
    payload = {
        "model": "sonar",
        "messages": [
            {"role": "system", "content": "×¢× ×” ×‘×§×¦×¨×” ×××•×“ ×‘×¢×‘×¨×™×ª."},
            {"role": "user", "content": f"""
            ×”×× ×”×“×’××™× ×”×‘××™× × ××›×¨×™× ×‘×™×©×¨××œ ×‘×™×“ ×©× ×™×™×”?
            {models_str}

            ×“×¨×™×©×•×ª: ×× ×•×¢ {engine} {engine_size} ×¡××´×§, ×’×™×¨ {gearbox}, {"×˜×•×¨×‘×•" if turbo=="×›×Ÿ" else "×œ×œ× ×˜×•×¨×‘×•"}, ×©× ×•×ª ×™×™×¦×•×¨ {year_range}.
            ×¦×™×™×Ÿ ×’× ×˜×•×•×—×™ ××—×™×¨×™×. 
            """}
        ]
    }
    answer = safe_perplexity_call(payload)
    debug_info["batch_filter"] = answer

    # × ×¡×” ×œ×–×”×•×ª ×¨×§ ×“×’××™× ×©× ××¦××• "× ××›×¨×™×"
    for m in models:
        if m.lower() in answer.lower() and ("× ×¤×•×¥" in answer or "× ××›×¨" in answer or "××—×™×¨" in answer):
            filtered.append(m)

    return filtered, debug_info

# =============================
# ×©×œ×‘ 3 â€“ × ×ª×•× ×™× ××œ××™× ×‘-Batch JSON
# =============================
def fetch_models_data_with_perplexity(models, answers):
    models_str = ", ".join(models)
    payload = {
        "model": "sonar-pro",
        "messages": [
            {"role": "system", "content": "×¢× ×” ×‘×¤×•×¨××˜ JSON ×‘×œ×‘×“. ××œ ×ª×•×¡×™×£ ×˜×§×¡×˜ ×—×•×¤×©×™."},
            {"role": "user", "content": f"""
            ×”×‘× ××™×“×¢ ×¢×“×›× ×™ ×¢×œ ×”×“×’××™× ×”×‘××™× ×‘×™×©×¨××œ: {models_str}.

            ×”×—×–×¨ ×ª×©×•×‘×” ×‘×¤×•×¨××˜ JSON ×¢× ×”××‘× ×”:
            {{
              "Model Name": {{
                 "price_range": "×˜×•×•×— ××—×™×¨×•×Ÿ ×××•×¦×¢ ×‘×™×“ ×©× ×™×™×”",
                 "availability": "×–××™× ×•×ª ×•× ×¤×•×¦×•×ª ×‘×™×©×¨××œ",
                 "insurance": "×¢×œ×•×ª ×‘×™×˜×•×— ×××•×¦×¢×ª",
                 "license_fee": "××’×¨×ª ×¨×™×©×•×™/×˜×¡×˜ ×©× ×ª×™×ª",
                 "maintenance": "×ª×—×–×•×§×” ×©× ×ª×™×ª ×××•×¦×¢×ª",
                 "common_issues": "×ª×§×œ×•×ª × ×¤×•×¦×•×ª ×™×“×•×¢×•×ª",
                 "fuel_consumption": "×¦×¨×™×›×ª ×“×œ×§ ×××™×ª×™×ª",
                 "depreciation": "×™×¨×™×“×ª ×¢×¨×š ×××•×¦×¢×ª",
                 "safety": "×“×™×¨×•×’ ×‘×˜×™×—×•×ª",
                 "parts_availability": "×–××™× ×•×ª ×—×œ×¤×™× ×‘×™×©×¨××œ"
              }}
            }}

            ×¢×‘×•×¨ ×›×œ ×“×’× ××”×¨×©×™××”.
            ××œ ×ª×•×¡×™×£ ×˜×§×¡×˜ ××¢×‘×¨ ×œ-JSON.
            """}
        ]
    }
    answer = safe_perplexity_call(payload)
    try:
        parsed = json.loads(answer)
    except Exception as e:
        parsed = {"error": str(e), "raw": answer}
    return parsed

# =============================
# ×©×œ×‘ 4 â€“ GPT ××¡×›× ×”××œ×¦×”
# =============================
def final_recommendation_with_gpt(answers, models, models_data):
    text = f"""
    ×ª×©×•×‘×•×ª ×”××©×ª××©:
    {answers}

    ×“×’××™× ×–××™× ×™× ×‘×™×©×¨××œ:
    {models}

    × ×ª×•× ×™ Perplexity (JSON):
    {models_data}

    ×¦×•×¨ ×”××œ×¦×” ×¡×•×¤×™×ª ×‘×¢×‘×¨×™×ª:
    - ×”×¦×’ ×¢×“ 5 ×“×’××™× ×‘×œ×‘×“
    - ×¤×¨×˜ ×™×ª×¨×•× ×•×ª ×•×—×¡×¨×•× ×•×ª
    - ×›×œ×•×œ × ×™××•×§×™× ××™×©×™×™× ×œ×¤×™ ×”×ª×§×¦×™×‘, ×¡×•×’ ×”×× ×•×¢, × ×¤×— ×× ×•×¢, ×©× ×•×ª ×™×™×¦×•×¨, ×˜×•×¨×‘×• ×•×’×™×¨
    """
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": text}],
        temperature=0.5,
    )
    return response.choices[0].message.content

# =============================
# Streamlit UI
# =============================
st.set_page_config(page_title="Car-Advisor", page_icon="ğŸš—")
st.title("ğŸš— Car-Advisor â€“ ×™×•×¢×¥ ×¨×›×‘×™× ×—×›×")

with st.form("car_form"):
    st.write("×¢× ×” ×¢×œ ×”×©××œ×•×Ÿ:")

    answers = {}
    answers["budget_range"] = st.selectbox("××” ×˜×•×•×— ×”×ª×§×¦×™×‘ ×©×œ×š ×œ×¨×›×‘?", ["5â€“10K", "10â€“20K", "20â€“40K", "40K+"])
    answers["budget_min"] = int(st.text_input("×ª×§×¦×™×‘ ××™× ×™××œ×™ (â‚ª)", "10000"))
    answers["budget_max"] = int(st.text_input("×ª×§×¦×™×‘ ××§×¡×™××œ×™ (â‚ª)", "20000"))
    answers["km"] = st.selectbox("×›××” ×§×™×œ×•××˜×¨×™× ××ª×” × ×•×¡×¢ ×‘×—×•×“×©?", ["<1000", "1000â€“2000", "2000â€“4000", "4000+"])
    answers["engine"] = st.radio("××™×–×” ×¡×•×’ ×× ×•×¢ ××ª×” ××¢×“×™×£?", ["×‘× ×–×™×Ÿ", "×“×™×–×œ", "×”×™×‘×¨×™×“×™", "×—×©××œ×™"])
    answers["engine_size"] = st.selectbox("××” × ×¤×— ×”×× ×•×¢ ×”××•×¢×“×£?", ["1200", "1600", "2000", "3000+"])
    answers["year_range"] = st.selectbox("××” ×©× ×•×ª ×”×™×™×¦×•×¨ ×”×¨×¦×•×™×•×ª?", ["2010â€“2015", "2016â€“2020", "2021+"])
    answers["usage"] = st.radio("××” ×”×©×™××•×© ×”×¢×™×§×¨×™ ×‘×¨×›×‘?", ["×¢×™×¨×•× ×™", "×‘×™×Ÿ-×¢×™×¨×•× ×™", "××¢×•×¨×‘"])
    answers["size"] = st.selectbox("××™×–×” ×’×•×“×œ ×¨×›×‘ ××ª××™× ×œ×š?", ["×§×˜×Ÿ", "××©×¤×—×ª×™", "SUV", "×˜× ×“×¨"])
    answers["car_type"] = st.selectbox("××™×–×” ×¡×•×’ ×¨×›×‘ ××ª××™× ×œ×š?", ["×¡×“××Ÿ", "×”××¦'×‘×§", "SUV", "×˜× ×“×¨", "××©×¤×—×ª×™"])
    answers["turbo"] = st.radio("××ª×” ××¢×“×™×£ ×× ×•×¢ ×¢× ×˜×•×¨×‘×•?", ["×œ× ××©× ×”", "×›×Ÿ", "×œ×"])
    answers["gearbox"] = st.radio("××™×–×” ×¡×•×’ ×ª×™×‘×ª ×”×™×œ×•×›×™× ××ª×” ××¢×“×™×£?", ["×œ× ××©× ×”", "××•×˜×•××˜", "×™×“× ×™", "×¨×•×‘×•×˜×™"])
    answers["passengers"] = st.radio("×›××” ×× ×©×™× × ×•×¡×¢×™× ×œ×¨×•×‘ ×‘×¨×›×‘?", ["1", "2â€“3", "4â€“5", "6+"])
    answers["fuel_eff"] = st.radio("×¢×“ ×›××” ×—×©×•×‘ ×—×¡×›×•×Ÿ ×‘×“×œ×§?", ["×œ× ×—×©×•×‘", "×‘×™× ×•× ×™", "×—×©×•×‘ ×××•×“"])
    answers["safety"] = st.radio("×¢×“ ×›××” ×—×©×•×‘×” ×¨××ª ×‘×˜×™×—×•×ª?", ["× ××•×š", "×‘×™× ×•× ×™", "×’×‘×•×” ×××•×“"])
    answers["extra"] = st.text_area("×™×© ××©×”×• × ×•×¡×£ ×©×—×©×•×‘ ×œ×¦×™×™×Ÿ?")

    submitted = st.form_submit_button("×©×œ×— ×•×§×‘×œ ×”××œ×¦×”")

if submitted:
    cached_result = get_from_cache(answers)
    if cached_result:
        st.success("âœ… ×”×ª×•×¦××” × ×˜×¢× ×” ××”×××’×¨ (Cache, 24 ×©×¢×•×ª)")
        summary = cached_result
    else:
        with st.spinner("ğŸ¤– GPT ×‘×•×—×¨ ×“×’××™× ×¨××©×•× ×™×™×..."):
            initial_models = analyze_needs_with_gpt(answers)
        st.info(f"ğŸ“‹ ×“×’××™× ×¨××©×•× ×™×™×: {initial_models}")

        with st.spinner("ğŸ‡®ğŸ‡± ××¡× ×Ÿ ×“×’××™× ××•×œ ××—×™×¨×•×Ÿ ×××™×ª×™ ×•×¡×•×’ ×× ×•×¢..."):
            israeli_models, debug_info = filter_models_for_israel(
                initial_models, answers["budget_min"], answers["budget_max"],
                answers["engine"], answers["gearbox"], answers["turbo"],
                answers["engine_size"], answers["year_range"]
            )

        with st.expander("ğŸ” ×ª×©×•×‘×•×ª Perplexity ×œ×¡×™× ×•×Ÿ"):
            st.write(debug_info)

        if not israeli_models:
            st.error("âŒ ×œ× × ××¦××• ×“×’××™× ×–××™× ×™× ×‘×™×©×¨××œ ×‘×”×ª×× ×œ×“×¨×™×©×•×ª.")
        else:
            st.success(f"âœ… ×“×’××™× ×–××™× ×™× ×‘×™×©×¨××œ: {israeli_models}")

            with st.spinner("ğŸŒ ×©×•×œ×£ × ×ª×•× ×™× ××œ××™× ×Ö¾Perplexity (Batch)..."):
                models_data = fetch_models_data_with_perplexity(israeli_models, answers)

            # ×˜×‘×œ×ª ×”×©×•×•××”
            try:
                df = pd.DataFrame(models_data).T
                st.subheader("ğŸ“Š ×”×©×•×•××ª × ×ª×•× ×™× ×‘×™×Ÿ ×”×“×’××™×")
                st.dataframe(df)
            except:
                st.warning("âš ï¸ ×œ× × ×™×ª×Ÿ ×œ×”×¦×™×’ ×˜×‘×œ×”, ×”× ×ª×•× ×™× ×œ× ×‘×¤×•×¨××˜ JSON ××œ×")
                st.write(models_data)

            with st.spinner("âš¡ ×™×•×¦×¨ ×”××œ×¦×” ×¡×•×¤×™×ª ×¢× GPT..."):
                summary = final_recommendation_with_gpt(answers, israeli_models, models_data)

            st.subheader("ğŸ” ×”×”××œ×¦×” ×”×¡×•×¤×™×ª ×©×œ×š")
            st.write(summary)

            save_to_cache(answers, summary)

    # ×”×¢×¨×•×ª ×—×©×•×‘×•×ª ×‘×¡×•×£
    st.markdown("---")
    st.markdown("âš ï¸ **×—×©×•×‘ ×œ×“×¢×ª:**")
    st.markdown("1. ××•××œ×¥ ×œ×‘×“×•×§ ××ª [×××’×¨ ×”×¢×‘×¨ ×”×‘×™×˜×•×—×™ ×©×œ ××¨×›×– ×”×¡×œ×™×§×”](https://www.cbc.org.il/) ×œ×§×‘×œ×ª ×”×™×¡×˜×•×¨×™×™×ª ×ª××•× ×•×ª ×¢×œ ×”×¨×›×‘.")
    st.markdown("2. ×¨×¦×•×™ ×œ×§×—×ª ××ª ×”×¨×›×‘ ×œ×‘×“×™×§×” ×‘××›×•×Ÿ ×‘×“×™×§×” ××•×¨×©×” ×œ×¤× ×™ ×¨×›×™×©×”.")
