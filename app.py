import os
import re
import requests
import streamlit as st
from openai import OpenAI

# =============================
# ×©×œ×™×¤×ª ××¤×ª×—×•×ª API
# =============================
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY")

if not OPENAI_API_KEY or not PERPLEXITY_API_KEY:
    st.error("âŒ ×œ× × ××¦××• ××¤×ª×—×•×ª API. ×•×“× ×©×”×’×“×¨×ª ××•×ª× ×‘×¡×™×§×¨×˜×¡.")
    st.stop()

client = OpenAI(api_key=OPENAI_API_KEY)

# =============================
# ×¤×•× ×§×¦×™×™×ª ×§×¨×™××” ×‘×˜×•×—×” ×œÖ¾Perplexity
# =============================
def safe_perplexity_call(payload):
    url = "https://api.perplexity.ai/chat/completions"
    headers = {"Authorization": f"Bearer {PERPLEXITY_API_KEY}", "Content-Type": "application/json"}
    try:
        r = requests.post(url, headers=headers, json=payload, timeout=60)
        data = r.json()
        if "choices" not in data:
            return f"×©×’×™××ª API: {data}"
        return data["choices"][0]["message"]["content"]
    except Exception as e:
        return f"×©×’×™××”: {e}"

# =============================
# ×©××œ×•×Ÿ â€“ 40 ×©××œ×•×ª (××§×•×¦×¨ ×›××Ÿ)
# =============================
questions = [
    "××” ×˜×•×•×— ×”×ª×§×¦×™×‘ ×©×œ×š ×œ×¨×›×‘?",
    "××” ×”×ª×§×¦×™×‘ ×”××™× ×™××œ×™ ×©×œ×š ×‘×©×§×œ×™×?",
    "××” ×”×ª×§×¦×™×‘ ×”××§×¡×™××œ×™ ×©×œ×š ×‘×©×§×œ×™×?",
    "×›××” ×§×™×œ×•××˜×¨×™× ××ª×” × ×•×¡×¢ ×‘×××•×¦×¢ ×‘×—×•×“×©?",
    "×”×× ×”×¨×›×‘ ××™×•×¢×“ ×‘×¢×™×§×¨ ×œ× ×¡×™×¢×•×ª ×¢×™×¨×•× ×™×•×ª ××• ×‘×™×Ÿ-×¢×™×¨×•× ×™×•×ª?",
    "×›××” ×× ×©×™× ×™×•×©×‘×™× ×‘×“×¨×š ×›×œ×œ ×‘×¨×›×‘?",
    "×”×× ××ª×” ×–×§×•×§ ×œ×ª× ××˜×¢×Ÿ ×’×“×•×œ?",
    "××ª×” ××ª×›× ×Ÿ ×œ× ×¡×•×¢ ×”×¨×‘×” ×¢× ×¦×™×•×“ ×›×‘×“ ××• ×’×¨×™×¨×”?",
    "××ª×” ××¢×“×™×£ ×¨×›×‘ ×‘× ×–×™×Ÿ, ×“×™×–×œ, ×”×™×‘×¨×™×“×™ ××• ×—×©××œ×™?",
    "×”×× ×—×¡×›×•×Ÿ ×‘×“×œ×§ ×§×¨×™×˜×™ ×¢×‘×•×¨×š?",
    # ... (×©××¨ ×”Ö¾40 ×©××œ×•×ª ×›××• ×§×•×“×)
]

# =============================
# ×¤×•× ×§×¦×™×•×ª Pipeline
# =============================

def analyze_needs_with_gpt(answers):
    """×©×œ×‘ 1 â€“ GPT: ×¨×©×™××ª ×“×’××™× ×¨××©×•× ×™×ª"""
    prompt = f"""
    ××œ×• ×”×ª×©×•×‘×•×ª ××”××©×ª××©:
    {answers}

    ×”×—×–×¨ ×¨×©×™××” ×©×œ 5â€“7 ×“×’××™ ×¨×›×‘×™× ××ª××™××™×.
    ×—×©×•×‘:
    - ×”×¦×¢ ×¨×§ ×¨×›×‘×™× ×©× ××›×¨×™× ×‘×™×©×¨××œ ×‘×¤×•×¢×œ (×—×“×©×™× ××• ×™×“ ×©× ×™×™×”).
    - ××œ ×ª×¦×™×¢ ×’×¨×¡××•×ª ×× ×•×¢/×ª×¦×•×¨×” ×©×œ× × ××›×¨×• ×‘×™×©×¨××œ.
    - ×”×—×–×¨ ×¨×©×™××” × ×§×™×™×”: ×¨×§ ×©× ×”×“×’×, ×›×œ ×“×’× ×‘×©×•×¨×” × ×¤×¨×“×ª, ×‘×œ×™ ××¡×¤×¨×™× ×•×‘×œ×™ ×”×¡×‘×¨×™×.
    """
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2,
    )
    text = response.choices[0].message.content
    clean_models = []
    for line in text.split("\n"):
        line = line.strip()
        if not line:
            continue
        line = re.sub(r"^[0-9\.\-\â€¢\*\s]+", "", line)
        if len(line.split()) <= 6:
            clean_models.append(line)
    return clean_models

def filter_models_for_israel(models):
    """×©×œ×‘ 2 â€“ ×¡×™× ×•×Ÿ ×œ×¤×™ ×–××™× ×•×ª ×‘×™×©×¨××œ + ××—×™×¨×•×Ÿ/×”×¢×¨×›×”"""
    filtered, debug_info = [], {}
    for model_name in models:
        payload = {
            "model": "sonar-medium-online",
            "messages": [
                {"role": "system", "content": "×¢× ×” ×‘×§×¦×¨×”, ×œ××©×œ: '× ×¤×•×¥ ×‘×™×©×¨××œ ×•×™×© ××—×™×¨×•×Ÿ', '× ×¤×•×¥ ×‘×™×©×¨××œ ×•×™×© ×”×¢×¨×›×ª ××—×™×¨', ××• '×œ× × ×¤×•×¥ ×‘×™×©×¨××œ'."},
                {"role": "user", "content": f"×”×× {model_name} × ××›×¨ ×‘×™×©×¨××œ ×‘×©×•×§ ×”×™×“ ×©× ×™×™×”, ×•×”×× ×™×© ×œ×• ××—×™×¨×•×Ÿ ××• ×œ×¤×—×•×ª ×”×¢×¨×›×ª ××—×™×¨?"}
            ]
        }
        answer = safe_perplexity_call(payload)
        debug_info[model_name] = answer
        if isinstance(answer, str):
            ans = answer.lower()
            if "×œ× × ×¤×•×¥" in ans or "×œ× × ××›×¨" in ans:
                continue
            if any(w in ans for w in ["× ×¤×•×¥", "× ××›×¨", "×§×™×™×", "×›×Ÿ"]) and any(w in ans for w in ["××—×™×¨", "×©×•×•×™", "×”×¢×¨×›×”"]):
                filtered.append(model_name)
    return filtered, debug_info

def fetch_models_data_with_perplexity(models, answers):
    """×©×œ×‘ 3 â€“ Perplexity: × ×ª×•× ×™× ××œ××™×"""
    all_data = {}
    for model_name in models:
        payload = {
            "model": "sonar-medium-online",
            "messages": [
                {"role": "system", "content": "×”×—×–×¨ ××™×“×¢ ×¢×•×‘×“×ª×™ ×•×ª××¦×™×ª×™ ×‘×œ×‘×“, ×‘×¢×‘×¨×™×ª."},
                {"role": "user", "content": f"""
                ×ª×©×•×‘×•×ª ×”××©×ª××©: {answers}

                ×”×‘× ××™×“×¢ ×¢×“×›× ×™ ×¢×œ {model_name} ×‘×™×©×¨××œ:
                - ××—×™×¨×•×Ÿ ×××•×¦×¢ ×œ×™×“ ×©× ×™×™×”
                - ×¢×œ×•×ª ×‘×™×˜×•×— ×××•×¦×¢×ª
                - ××’×¨×ª ×¨×™×©×•×™ ×•×˜×¡×˜ ×©× ×ª×™×ª
                - ×¢×œ×•×ª ×˜×™×¤×•×œ×™× ×©× ×ª×™×ª ×××•×¦×¢×ª
                - ×ª×§×œ×•×ª × ×¤×•×¦×•×ª
                - ×¦×¨×™×›×ª ×“×œ×§ ×××™×ª×™×ª
                - ×™×¨×™×“×ª ×¢×¨×š ×××•×¦×¢×ª
                - ×“×™×¨×•×’ ×‘×˜×™×—×•×ª
                - ×–××™× ×•×ª ×—×œ×¤×™× ×•×¢×œ×•×ª×
                - ×‘×™×§×•×© ×‘×©×•×§ ×”×™×“ ×©× ×™×™×”
                """}
            ]
        }
        answer = safe_perplexity_call(payload)
        all_data[model_name] = answer
    return all_data

def final_recommendation_with_gpt(answers, models, models_data):
    """×©×œ×‘ 4 â€“ GPT: ×”××œ×¦×” ×¡×•×¤×™×ª"""
    text = f"""
    ×ª×©×•×‘×•×ª ×”××©×ª××©:
    {answers}

    ×“×’××™× ×–××™× ×™× ×‘×™×©×¨××œ:
    {models}

    × ×ª×•× ×™ Perplexity:
    {models_data}

    ×¦×•×¨ ×”××œ×¦×” ×¡×•×¤×™×ª ×‘×¢×‘×¨×™×ª:
    - ×”×¦×’ ×¢×“ 5 ×“×’××™× ×‘×œ×‘×“
    - ×”×•×¡×£ × ×™××•×§ ××™×©×™ ×œ×›×œ ×“×’×
    - ×”×©×•×•×” ×™×ª×¨×•× ×•×ª ×•×—×¡×¨×•× ×•×ª
    - ×›×œ×•×œ ×©×™×§×•×œ×™× ×›×œ×›×œ×™×™× ×•×©×™××•×©×™×™×
    """
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": text}],
        temperature=0.5,
    )
    return response.choices[0].message.content

# =============================
# Streamlit UI
# =============================
st.set_page_config(page_title="Car-Advisor", page_icon="ğŸš—")
st.title("ğŸš— Car-Advisor â€“ ×™×•×¢×¥ ×¨×›×‘×™× ×—×›×")

with st.form("car_form"):
    st.write("×¢× ×” ×¢×œ ×”×©××œ×•×Ÿ:")
    answers = {}
    for q in questions:
        answers[q] = st.text_input(q, "")
    submitted = st.form_submit_button("×©×œ×— ×•×§×‘×œ ×”××œ×¦×”")

if submitted:
    with st.spinner("ğŸ¤– GPT ×‘×•×—×¨ ×“×’××™× ×¨××©×•× ×™×™×..."):
        initial_models = analyze_needs_with_gpt(answers)
    st.info(f"ğŸ“‹ ×“×’××™× ×¨××©×•× ×™×™×: {initial_models}")

    with st.spinner("ğŸ‡®ğŸ‡± ××¡× ×Ÿ ×“×’××™× (×™×©×¨××œ + ××—×™×¨×•×Ÿ/×”×¢×¨×›×”)..."):
        israeli_models, debug_info = filter_models_for_israel(initial_models)

    with st.expander("ğŸ” ×ª×©×•×‘×•×ª Perplexity ×œ×¡×™× ×•×Ÿ"):
        st.write(debug_info)

    if not israeli_models:
        st.error("âŒ ×œ× × ××¦××• ×“×’××™× ×–××™× ×™× ×‘×™×©×¨××œ ×¢× ××—×™×¨×•×Ÿ ××• ×”×¢×¨×›×ª ××—×™×¨.")
    else:
        st.success(f"âœ… ×“×’××™× ×–××™× ×™× ×‘×™×©×¨××œ: {israeli_models}")

        with st.spinner("ğŸŒ ×©×•×œ×£ × ×ª×•× ×™× ××œ××™× ×Ö¾Perplexity..."):
            models_data = fetch_models_data_with_perplexity(israeli_models, answers)

        with st.expander("ğŸ“Š × ×ª×•× ×™ Perplexity ×’×•×œ××™×™×"):
            st.write(models_data)

        with st.spinner("âš¡ ×™×•×¦×¨ ×”××œ×¦×” ×¡×•×¤×™×ª ×¢× GPT..."):
            summary = final_recommendation_with_gpt(answers, israeli_models, models_data)

        st.subheader("ğŸ” ×”×”××œ×¦×” ×”×¡×•×¤×™×ª ×©×œ×š")
        st.write(summary)
